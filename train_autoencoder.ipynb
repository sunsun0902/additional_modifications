{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder Training Pipeline\n",
    "\n",
    "This notebook implements a complete pipeline for training a convolutional autoencoder on weather data using Bayesian optimization for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "from bayesian_tuner import MLModel\n",
    "from data_processing import process_epw_to_parquet_with_dask\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Weather Data (if not already exists)\n",
    "\n",
    "**Note:** Update the paths below to match your actual EPW files location and desired output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if weather_data.parquet already exists\n",
    "parquet_file_path = \"weather_data.parquet\"\n",
    "\n",
    "if not os.path.exists(parquet_file_path):\n",
    "    print(\"Weather data parquet file not found. Generating from EPW files...\")\n",
    "    \n",
    "    # Update these paths to match your system\n",
    "    epw_directory = \"path/to/your/epw/files\"  # UPDATE THIS PATH\n",
    "    output_path = \"weather_data.parquet\"\n",
    "    \n",
    "    # Generate the parquet file using your data processing function\n",
    "    process_epw_to_parquet_with_dask(epw_directory, output_path)\n",
    "    print(f\"Generated {parquet_file_path}\")\n",
    "else:\n",
    "    print(f\"Found existing parquet file: {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Explore the Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned weather data\n",
    "print(\"Loading weather data from parquet file...\")\n",
    "df = pd.read_parquet(parquet_file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Unique locations: {df['location'].nunique()}\")\n",
    "print(f\"Location names: {df['location'].unique()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check data types and missing values\n",
    "print(\"\\nData info:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric features (exclude timestamp and location)\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'timestamp' in numeric_columns:\n",
    "    numeric_columns.remove('timestamp')\n",
    "\n",
    "print(f\"Selected numeric features ({len(numeric_columns)}): {numeric_columns}\")\n",
    "\n",
    "# Extract numeric data for each location\n",
    "locations = df['location'].unique()\n",
    "location_data = []\n",
    "\n",
    "for location in locations:\n",
    "    location_df = df[df['location'] == location][numeric_columns]\n",
    "    # Ensure we have exactly 8760 hours (1 year)\n",
    "    if len(location_df) >= 8760:\n",
    "        location_df = location_df.iloc[:8760]\n",
    "    else:\n",
    "        print(f\"Warning: {location} has only {len(location_df)} hours, padding to 8760\")\n",
    "        # Pad with the last known values\n",
    "        last_row = location_df.iloc[-1:]\n",
    "        padding_needed = 8760 - len(location_df)\n",
    "        padding_df = pd.concat([last_row] * padding_needed, ignore_index=True)\n",
    "        location_df = pd.concat([location_df, padding_df], ignore_index=True)\n",
    "    \n",
    "    location_data.append(location_df.values)\n",
    "\n",
    "# Stack data for all locations into a single array\n",
    "# Shape: (num_locations, 8760, num_features)\n",
    "weather_data = np.array(location_data)\n",
    "print(f\"Stacked weather data shape: {weather_data.shape}\")\n",
    "\n",
    "# Normalize the data using MinMaxScaler\n",
    "print(\"Normalizing data with MinMaxScaler...\")\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Reshape for scaling: (num_locations * 8760, num_features)\n",
    "original_shape = weather_data.shape\n",
    "weather_data_reshaped = weather_data.reshape(-1, weather_data.shape[-1])\n",
    "\n",
    "# Fit and transform\n",
    "weather_data_normalized = scaler.fit_transform(weather_data_reshaped)\n",
    "\n",
    "# Reshape back to original shape\n",
    "weather_data_normalized = weather_data_normalized.reshape(original_shape)\n",
    "\n",
    "print(f\"Normalized data shape: {weather_data_normalized.shape}\")\n",
    "print(f\"Data range after normalization: [{weather_data_normalized.min():.3f}, {weather_data_normalized.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Data Shape for Conv1D Compatibility (Updated for ResNet)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# The new ResNet architecture uses Conv1D layers, so we don't need the extra channel dimension\n# Data is already in the correct shape (num_locations, 8760, num_features)\nweather_data_conv = weather_data_normalized\n\nprint(f\"Data shape for Conv1D ResNet: {weather_data_conv.shape}\")\nprint(f\"Input shape for autoencoder: {weather_data_conv.shape[1:]}\")\n\n# Store input shape for model building\ninput_shape = weather_data_conv.shape[1:]  # (8760, num_features)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "# Using 80% for training, 20% for testing\n",
    "X_train, X_test = train_test_split(\n",
    "    weather_data_conv, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Setup Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories and model configuration\n",
    "tuner_directory = \"autoencoder_tuner\"\n",
    "project_name = \"weather_autoencoder_optimization\"\n",
    "model_path = \"trained_models\"\n",
    "model_name = \"weather_autoencoder.h5\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(tuner_directory, exist_ok=True)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "print(f\"Tuner directory: {tuner_directory}\")\n",
    "print(f\"Project name: {project_name}\")\n",
    "print(f\"Model path: {model_path}\")\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"Input shape: {input_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Initialize MLModel and Start Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLModel class for autoencoder\n",
    "ml_model = MLModel(\n",
    "    tuner_directory=tuner_directory,\n",
    "    project_name=project_name,\n",
    "    path_model=model_path,\n",
    "    model_name=model_name,\n",
    "    input_shape=input_shape,\n",
    "    model_type='Autoencoder'\n",
    ")\n",
    "\n",
    "print(\"MLModel initialized successfully for autoencoder training\")\n",
    "print(f\"Model type: {ml_model.model_type}\")\n",
    "print(f\"Input shape: {ml_model.input_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Run Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Bayesian optimization process\n",
    "# Note: This will take a considerable amount of time depending on max_trials\n",
    "print(\"Starting Bayesian optimization for autoencoder hyperparameters...\")\n",
    "print(\"This process may take several hours depending on the number of trials.\")\n",
    "\n",
    "# For autoencoders, we pass the same data as input and target\n",
    "ml_model.tune_model(\n",
    "    train_x=X_train,\n",
    "    train_y=X_train,  # In autoencoders, target = input\n",
    "    epochs=50  # Reduced for faster tuning, increase if needed\n",
    ")\n",
    "\n",
    "print(\"Hyperparameter tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Get Best Model and Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from tuning results\n",
    "print(\"Retrieving best autoencoder model...\")\n",
    "best_model = ml_model.get_best_model(\n",
    "    train_x=X_train,\n",
    "    train_y=X_train,  # Target = input for autoencoder\n",
    "    val_x=X_test,\n",
    "    val_y=X_test     # Target = input for autoencoder\n",
    ")\n",
    "\n",
    "print(\"Best model retrieved and trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Evaluate Reconstruction Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the autoencoder's reconstruction performance on test set\n",
    "print(\"Evaluating reconstruction performance on test set...\")\n",
    "\n",
    "# Get reconstructions from the trained model\n",
    "X_test_reconstructed = best_model.predict(X_test)\n",
    "\n",
    "# Calculate reconstruction metrics\n",
    "mse_per_sample = np.mean(np.square(X_test - X_test_reconstructed), axis=(1, 2, 3))\n",
    "mae_per_sample = np.mean(np.abs(X_test - X_test_reconstructed), axis=(1, 2, 3))\n",
    "\n",
    "# Overall metrics\n",
    "overall_mse = np.mean(mse_per_sample)\n",
    "overall_mae = np.mean(mae_per_sample)\n",
    "overall_rmse = np.sqrt(overall_mse)\n",
    "\n",
    "print(f\"\\nReconstruction Performance Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {overall_mse:.6f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {overall_rmse:.6f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {overall_mae:.6f}\")\n",
    "\n",
    "print(f\"\\nPer-sample statistics:\")\n",
    "print(f\"MSE - Min: {np.min(mse_per_sample):.6f}, Max: {np.max(mse_per_sample):.6f}, Std: {np.std(mse_per_sample):.6f}\")\n",
    "print(f\"MAE - Min: {np.min(mae_per_sample):.6f}, Max: {np.max(mae_per_sample):.6f}, Std: {np.std(mae_per_sample):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history if available\n",
    "try:\n",
    "    ml_model.plot_loss()\n",
    "except:\n",
    "    print(\"Could not plot training history\")\n",
    "\n",
    "# Plot reconstruction error distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(mse_per_sample, bins=30, alpha=0.7, color='blue')\n",
    "plt.xlabel('MSE per Sample')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of MSE per Sample')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(mae_per_sample, bins=30, alpha=0.7, color='green')\n",
    "plt.xlabel('MAE per Sample')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of MAE per Sample')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize original vs reconstructed data for a sample\n",
    "sample_idx = 0\n",
    "feature_idx = 0  # Choose which feature to visualize\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original data\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(X_test[sample_idx, :, feature_idx, 0])\n",
    "plt.title(f'Original Data (Sample {sample_idx}, Feature {feature_idx})')\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Normalized Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Reconstructed data\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(X_test_reconstructed[sample_idx, :, feature_idx, 0])\n",
    "plt.title(f'Reconstructed Data (Sample {sample_idx}, Feature {feature_idx})')\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Normalized Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(X_test[sample_idx, :, feature_idx, 0], label='Original', alpha=0.8)\n",
    "plt.plot(X_test_reconstructed[sample_idx, :, feature_idx, 0], label='Reconstructed', alpha=0.8)\n",
    "plt.title(f'Comparison (Sample {sample_idx}, Feature {feature_idx})')\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Normalized Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSample {sample_idx} reconstruction MSE: {mse_per_sample[sample_idx]:.6f}\")\n",
    "print(f\"Sample {sample_idx} reconstruction MAE: {mae_per_sample[sample_idx]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Save Results and Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save reconstruction metrics\n",
    "results = {\n",
    "    'overall_mse': float(overall_mse),\n",
    "    'overall_rmse': float(overall_rmse),\n",
    "    'overall_mae': float(overall_mae),\n",
    "    'mse_per_sample_stats': {\n",
    "        'min': float(np.min(mse_per_sample)),\n",
    "        'max': float(np.max(mse_per_sample)),\n",
    "        'mean': float(np.mean(mse_per_sample)),\n",
    "        'std': float(np.std(mse_per_sample))\n",
    "    },\n",
    "    'mae_per_sample_stats': {\n",
    "        'min': float(np.min(mae_per_sample)),\n",
    "        'max': float(np.max(mae_per_sample)),\n",
    "        'mean': float(np.mean(mae_per_sample)),\n",
    "        'std': float(np.std(mae_per_sample))\n",
    "    },\n",
    "    'model_info': {\n",
    "        'input_shape': input_shape,\n",
    "        'num_locations': len(locations),\n",
    "        'num_features': len(numeric_columns),\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to JSON file\n",
    "import json\n",
    "results_file = os.path.join(model_path, 'autoencoder_results.json')\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {results_file}\")\n",
    "\n",
    "# Save the scaler for future use\n",
    "import joblib\n",
    "scaler_file = os.path.join(model_path, 'weather_data_scaler.pkl')\n",
    "joblib.dump(scaler, scaler_file)\n",
    "print(f\"Data scaler saved to: {scaler_file}\")\n",
    "\n",
    "print(\"\\n=== TRAINING COMPLETE ===\")\n",
    "print(f\"Best autoencoder model saved to: {os.path.join(model_path, model_name)}\")\n",
    "print(f\"Overall reconstruction MSE: {overall_mse:.6f}\")\n",
    "print(f\"Overall reconstruction MAE: {overall_mae:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}